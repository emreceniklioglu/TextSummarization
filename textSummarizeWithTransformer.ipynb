{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-12T15:22:12.955268Z",
          "iopub.status.busy": "2023-07-12T15:22:12.954829Z",
          "iopub.status.idle": "2023-07-12T15:22:21.691466Z",
          "shell.execute_reply": "2023-07-12T15:22:21.690200Z",
          "shell.execute_reply.started": "2023-07-12T15:22:12.955227Z"
        },
        "id": "Zbxhyl_zFlWL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, LayerNormalization, Layer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.train import Checkpoint, CheckpointManager\n",
        "from tensorflow.keras.metrics import Mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95wbnSFRR3Ax",
        "outputId": "5d58ad67-fdb6-430a-94a3-d55bd3c646b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5xwDWHii4Qd",
        "outputId": "9821fbe7-0f0d-4595-85af-293614eff627"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH5cg5pSIHaZ"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-12T15:22:21.694108Z",
          "iopub.status.busy": "2023-07-12T15:22:21.693308Z"
        },
        "id": "K_AjGkWXITKA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "data = pd.read_excel('/content/drive/MyDrive/Inshorts Cleaned Data.xlsx', sheet_name='Sheet1', usecols=[0,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "oXtxc-toIc94",
        "outputId": "c73dfab7-0559-413e-ea91-15118db39034",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Indian spinner Ravichandran Ashwin has broken the record for most wickets in a single Test season, taking his 79th this season during the Dharamsala Test against Australia on Saturday. Ashwin went past South African pacer Dale Steyn, who had claimed 78 wickets in 12 Tests in 2007-08. Ashwin has taken seven five-wicket hauls this season in 13 matches.'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['Short'][100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vR2hg9themaN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import unicodedata\n",
        "def clean_words(sentence):\n",
        "    sentence = str(sentence).lower()\n",
        "    sentence = unicodedata.normalize('NFKD', sentence).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "\n",
        "    # URL'leri temizle\n",
        "    sentence = re.sub(r\"http[s]?://\\S+\", \"\", sentence)\n",
        "\n",
        "    # Özel karakterler ve rakamlar\n",
        "    sentence = re.sub(r\"[\\[\\]\\\\0-9()\\\"$#%/@;:<>{}`+=~|.!?,-]|\\bcnn\\b\", \"\", sentence)\n",
        "\n",
        "    # Diğer özel karakterler\n",
        "    sentence = re.sub(r\"[&…•♦◆★☆■□▪▫▶◀▲▼]\", \"\", sentence)\n",
        "\n",
        "    # Ekstra boşlukları temizle\n",
        "    sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
        "\n",
        "    # Yeni satırları temizle\n",
        "    sentence = re.sub(r\"\\\\n\", \"\", sentence)\n",
        "\n",
        "    # Baş ve sondaki boşlukları kaldır\n",
        "    sentence = sentence.strip()\n",
        "\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4cEp3wmI2BX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "news = data['Short'].apply(clean_words)\n",
        "summary = data['Headline'].apply(clean_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z55AhpKIdK7",
        "outputId": "91c6c682-4b32-4e31-b531-147b02a7d455",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('indian spinner ravichandran ashwin has broken the record for most wickets in a single test season taking his th this season during the dharamsala test against australia on saturday ashwin went past south african pacer dale steyn who had claimed wickets in tests in ashwin has taken seven fivewicket hauls this season in matches',\n",
              " 'ashwin breaks record for most wickets in a test season')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "news[100], summary[100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8gKyq1gIq4r"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ6LE4MrJjC_",
        "outputId": "c2cff9a3-3603-4651-9824-683489e9f4e9",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary after adding tokens:\n",
            "0        <START> exbank officials booked for cheating b...\n",
            "1        <START> supreme court to go paperless in month...\n",
            "2        <START> at least killed injured in blast in sy...\n",
            "3        <START> why has reliance been barred from trad...\n",
            "4        <START> was stopped from entering my own studi...\n",
            "                               ...                        \n",
            "55099    <START> sensex loses points to hit week low <END>\n",
            "55100    <START> china to inject bn into the money mark...\n",
            "55101    <START> ghulam ali set to make acting debut in...\n",
            "55102    <START> is acknowledges death of jihadi john r...\n",
            "55103    <START> cairn to seek mn from india in damages...\n",
            "Name: Headline, Length: 55104, dtype: object\n"
          ]
        }
      ],
      "source": [
        "def add_tokens(x):\n",
        "    return '<START> ' + x + ' <END>'\n",
        "\n",
        "summary = summary.apply(add_tokens)\n",
        "print(\"Summary after adding tokens:\")\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95Zv7FIvKbTi"
      },
      "source": [
        "#### Tokenizing the texts into integer tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TqbpEyPMRqa",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# since < and > from default tokens cannot be removed\n",
        "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
        "oov_token = '<unk>'\n",
        "\n",
        "news_tokenizer = Tokenizer(oov_token=oov_token)\n",
        "summary_tokenizer = Tokenizer(filters=filters, oov_token=oov_token)\n",
        "\n",
        "news_tokenizer.fit_on_texts(news)\n",
        "summary_tokenizer.fit_on_texts(summary)\n",
        "\n",
        "inputs = news_tokenizer.texts_to_sequences(news)\n",
        "targets = summary_tokenizer.texts_to_sequences(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVyErXAei5_b",
        "outputId": "6e161d79-43e1-45fd-a0e0-17e0e7269c45",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[153, 20, 9, 62]]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary_tokenizer.texts_to_sequences([\"This is a test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ryx9qx90jwXu",
        "outputId": "1a350cd6-3d6f-412b-9326-3ea43227ada7",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['this is a test']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary_tokenizer.sequences_to_texts([[153, 20, 9, 62]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoizyBvLKv8h",
        "outputId": "57d366c6-152c-4964-8675-8dede54f9834",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(81290, 30897)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoder_vocab_size = len(news_tokenizer.word_index) + 1\n",
        "decoder_vocab_size = len(summary_tokenizer.word_index) + 1\n",
        "\n",
        "# vocab_size\n",
        "encoder_vocab_size, decoder_vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZden_q9_eZr"
      },
      "source": [
        "#### Obtaining insights on lengths for defining maxlen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ma4o2nGdK5Xb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "news_lengths = []\n",
        "summary_lengths = []\n",
        "\n",
        "for n,s in zip(news, summary):\n",
        "    news_lengths.append(len(n))\n",
        "    summary_lengths.append(len(s))\n",
        "\n",
        "\n",
        "\n",
        "news_lengths = pd.Series(news_lengths)\n",
        "summary_lengths = pd.Series(summary_lengths)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "iXZlO99C-UXK",
        "outputId": "cd4f265e-c750-42e0-c649-fde288ac37c1",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>55104.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>342.000036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>25.969631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>206.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>325.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>343.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>361.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>408.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ],
            "text/plain": [
              "count    55104.000000\n",
              "mean       342.000036\n",
              "std         25.969631\n",
              "min        206.000000\n",
              "25%        325.000000\n",
              "50%        343.000000\n",
              "75%        361.000000\n",
              "max        408.000000\n",
              "dtype: float64"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "news_lengths.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "ALMwKMx--ZF7",
        "outputId": "37b12b25-20de-4395-c5b4-c3e4ca5b9681",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>55104.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>61.665251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>6.731527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>15.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>57.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>61.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>67.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>79.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ],
            "text/plain": [
              "count    55104.000000\n",
              "mean        61.665251\n",
              "std          6.731527\n",
              "min         15.000000\n",
              "25%         57.000000\n",
              "50%         61.000000\n",
              "75%         67.000000\n",
              "max         79.000000\n",
              "dtype: float64"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary_lengths.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVeMilXr-bpC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "encoder_maxlen = 400\n",
        "decoder_maxlen = 75"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SWap3YJBk-D"
      },
      "source": [
        "#### Padding/Truncating sequences for identical sequence lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEyUBeu7ACRt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "inputs = pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "targets = pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIP0kIIcB8Rm"
      },
      "source": [
        "### Creating ds pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzO6l3-AB7hJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "inputs = tf.cast(inputs, dtype=tf.int32)\n",
        "targets = tf.cast(targets, dtype=tf.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slZ5f4P4DurS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wI-fV7eABWN6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgWDliiTedJS",
        "outputId": "bbcbba5a-7570-4e44-ca28-b255e58d85c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset created:\n",
            "Number of batches in the dataset: tf.Tensor(861, shape=(), dtype=int64)\n",
            "Batch size: 64\n"
          ]
        }
      ],
      "source": [
        "print(\"Dataset created:\")\n",
        "print(\"Number of batches in the dataset:\", tf.data.experimental.cardinality(ds))\n",
        "print(\"Batch size:\", BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isN1CpAXLfsl"
      },
      "source": [
        "### Positional Encoding for adding notion of position among words as unlike RNN this is non-directional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Purv7oyhETDZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_angles(position, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return position * angle_rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40J2pc2NEXp5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    position = np.arange(position)[:, np.newaxis]\n",
        "    i = np.arange(d_model)[np.newaxis, :]\n",
        "\n",
        "    exponent = (2 * (i // 2)) / np.float32(d_model)\n",
        "    angle_rates = 1 / np.power(10000, exponent)\n",
        "    angle_rads = position * angle_rates\n",
        "    angle_rads[:, ::2] = np.sin(angle_rads[:, ::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = np.expand_dims(angle_rads, axis=0)\n",
        "    pos_encoding = tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "    return pos_encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Pe01DMMWHc"
      },
      "source": [
        "### Masking\n",
        "\n",
        "- Padding mask for masking \"pad\" sequences\n",
        "- Lookahead mask for masking future words from contributing in prediction of current words in self attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2RXbjg76FL4"
      },
      "outputs": [],
      "source": [
        "def create_padding_mask(seq):\n",
        "    padding_mask = tf.math.equal(seq, 0)\n",
        "    padding_mask = tf.cast(padding_mask, tf.float32)\n",
        "\n",
        "    padding_mask = tf.expand_dims(padding_mask, axis=1)\n",
        "    padding_mask = tf.expand_dims(padding_mask, axis=2)\n",
        "    return padding_mask\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    ones = tf.ones((size, size))\n",
        "\n",
        "    req_matrix = tf.linalg.band_part(ones, -1, 0)\n",
        "    toggle_req_matrix = 1 - req_matrix\n",
        "    mask = toggle_req_matrix\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8DqUBc4NFOy"
      },
      "source": [
        "### Building the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfknVF7hNKf7"
      },
      "source": [
        "#### Scaled Dot Product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_B6M9OBNBKB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled += (mask * -1e9)\n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled, axis=-1)\n",
        "\n",
        "    values = tf.matmul(attention_weights, v)\n",
        "    return values, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf7_a5uQOfJk"
      },
      "source": [
        "#### Multi-Headed Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIuFrdXnNZEC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.head_dim = d_model // self.num_heads\n",
        "\n",
        "        self.wq = Dense(d_model)\n",
        "        self.wk = Dense(d_model)\n",
        "        self.wv = Dense(d_model)\n",
        "\n",
        "        self.linear_dense = Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim))\n",
        "\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, q, k, v, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        values, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "        values = tf.transpose(values, perm=[0, 2, 1, 3])\n",
        "        concat_values = tf.reshape(values, (batch_size, -1, self.d_model))\n",
        "\n",
        "        output = self.linear_dense(concat_values)\n",
        "\n",
        "        return output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A49tXMVvOkOZ"
      },
      "source": [
        "### Feed Forward Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9-qoKuTNwKq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def point_wise_feed_forward_network(d_model, hidden):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(hidden, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDUolDfP66vK"
      },
      "outputs": [],
      "source": [
        "class PointwiseFeedForward(Layer):\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PointwiseFeedForward, self).__init__()\n",
        "        self.linear1 = Dense(hidden, activation='relu')\n",
        "        self.linear2 = Dense(d_model)\n",
        "        self.dropout = Dropout(rate=drop_prob)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2RRmn2bOpW9"
      },
      "source": [
        "#### Fundamental Unit of Transformer encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNuoJoFWO335",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(Layer):\n",
        "    def __init__(self, d_model, num_heads, hidden, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.ffn = PointwiseFeedForward(d_model, hidden, drop_prob=rate)\n",
        "        self.norm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, x, *, training=False, mask=None):\n",
        "        residual = x\n",
        "        # self‐attention\n",
        "        x, _ = self.attention(x, x, x, mask)\n",
        "        x = self.dropout1(x, training=training)\n",
        "        x = self.norm1(residual + x)\n",
        "\n",
        "        residual = x\n",
        "        # feed‐forward\n",
        "        x = self.ffn(x)\n",
        "        x = self.dropout2(x, training=training)\n",
        "        x = self.norm2(residual + x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i6Zh8gnPqdW"
      },
      "source": [
        "#### Fundamental Unit of Transformer decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CVmvs6dPMRC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(Layer):\n",
        "    def __init__(self, d_model, num_heads, hidden, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.attention1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.attention2 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = PointwiseFeedForward(d_model, hidden, drop_prob=rate)\n",
        "        self.norm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.norm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.norm3 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "        self.dropout3 = Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, *, training=False, look_ahead_mask=None, padding_mask=None):\n",
        "        # 1. masked self-attention\n",
        "        attn1, attn_weights_block1 = self.attention1(\n",
        "            x, x, x, look_ahead_mask\n",
        "        )\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.norm1(attn1 + x)\n",
        "\n",
        "        # 2. encoder-decoder attention\n",
        "        attn2, attn_weights_block2 = self.attention2(\n",
        "            out1, enc_output, enc_output, padding_mask\n",
        "        )\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.norm2(attn2 + out1)\n",
        "\n",
        "        # 3. feed-forward\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.norm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zt5MUc_QNid"
      },
      "source": [
        "#### Encoder consisting of multiple EncoderLayer(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrbnTwijQJ-h",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Encoder(Layer):\n",
        "    def __init__(self,  d_model, num_layers, num_heads, hidden,\n",
        "                 input_vocab_size, max_pos_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(max_pos_encoding, d_model)\n",
        "        self.enc_layers = [\n",
        "            EncoderLayer(d_model, num_heads, hidden, rate)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "    def call(self, x, *, training=False, mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        for enc_layer in self.enc_layers:\n",
        "            x = enc_layer(x, training=training, mask=mask)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N5LrNrvRexg"
      },
      "source": [
        "#### Decoder consisting of multiple DecoderLayer(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmeqkZrIRbSB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Decoder(Layer):\n",
        "    def __init__(self, d_model, num_layers, num_heads, hidden,\n",
        "                 target_vocab_size, max_pos_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(max_pos_encoding, d_model)\n",
        "        self.dec_layers = [\n",
        "            DecoderLayer(d_model, num_heads, hidden, rate)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "        self.dropout = Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, *, training=False, look_ahead_mask=None, padding_mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i, dec_layer in enumerate(self.dec_layers):\n",
        "            x, block1, block2 = dec_layer(\n",
        "                x,\n",
        "                enc_output,\n",
        "                training=training,\n",
        "                look_ahead_mask=look_ahead_mask,\n",
        "                padding_mask=padding_mask\n",
        "            )\n",
        "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "        return x, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbMNK_bzSHnh"
      },
      "source": [
        "#### Finally, the Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXHRG-o4R9Mc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Transformer(Model):\n",
        "    def __init__(self, d_model, num_layers, num_heads, hidden,\n",
        "                 input_vocab_size, target_vocab_size,\n",
        "                 max_pos_input, max_pos_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(d_model, num_layers, num_heads, hidden,\n",
        "                               input_vocab_size, max_pos_input, rate)\n",
        "        self.decoder = Decoder(d_model, num_layers, num_heads, hidden,\n",
        "                               target_vocab_size, max_pos_target, rate)\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    # <-- Burada * ekleyip tüm non-tensor argümanları keyword-only yaptık\n",
        "    def call(\n",
        "        self,\n",
        "        inp,\n",
        "        tar,\n",
        "        *,\n",
        "        training=False,\n",
        "        enc_padding_mask=None,\n",
        "        look_ahead_mask=None,\n",
        "        dec_padding_mask=None\n",
        "    ):\n",
        "        # encoder ve decoder çağrılarını da keyword argümanlarla yapıyoruz\n",
        "        enc_output = self.encoder(\n",
        "            inp,\n",
        "            training=training,\n",
        "            mask=enc_padding_mask\n",
        "        )\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar,\n",
        "            enc_output,\n",
        "            training=training,\n",
        "            look_ahead_mask=look_ahead_mask,\n",
        "            padding_mask=dec_padding_mask\n",
        "        )\n",
        "        final_output = self.final_layer(dec_output)\n",
        "        return final_output, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UndsMPZXTdSr"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMTZJdIoSbuy",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# hyper-params\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "hidden = 512\n",
        "num_heads = 8\n",
        "EPOCHS = 40"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOGvkYDNTjIj"
      },
      "source": [
        "#### Adam optimizer with custom learning rate scheduling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfiynCLlTL8C",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000): # lrate = d_model^-0.5 * min(step_num^-0.5, step_num * warmup_steps^-1.5)\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, dtype=tf.float32)\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step* (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsVdrENTUERY"
      },
      "source": [
        "#### Defining losses and other metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ip1-943kTXXK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktKwyvKtTvF6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uW4LA_45T4Aa",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ze0u6xxXT7dI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XvKy3v6ULnO"
      },
      "source": [
        "#### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5-RcxqFUCuk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    d_model,\n",
        "    num_layers,\n",
        "    num_heads,\n",
        "    hidden,\n",
        "    encoder_vocab_size,\n",
        "    decoder_vocab_size,\n",
        "    max_pos_input=encoder_vocab_size,\n",
        "    max_pos_target=decoder_vocab_size,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f56BGiVXU_Dk"
      },
      "source": [
        "#### Masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZxHuyZxU5Pa",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_masks(inputs, targets):\n",
        "    enc_padding_mask = create_padding_mask(inputs)\n",
        "    dec_padding_mask = create_padding_mask(inputs)\n",
        "\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(targets)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(targets)\n",
        "\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYIotvaBVI0d"
      },
      "source": [
        "#### Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOc1_3c-VGaL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"checkpoints2\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print('Latest checkpoint restored!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfpI0gS4c06c"
      },
      "source": [
        "#### Training steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmVOMzkrczgl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Pass keyword arguments explicitly\n",
        "        predictions, _ = transformer(\n",
        "    inp,\n",
        "    tar_inp,\n",
        "    training=True,\n",
        "    enc_padding_mask=enc_padding_mask,\n",
        "    look_ahead_mask=combined_mask,\n",
        "    dec_padding_mask=dec_padding_mask\n",
        ")\n",
        "\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2J5R8m6qhtL",
        "outputId": "241a12a8-f130-488c-830f-66d4472c0c0c",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 10.3489\n",
            "Epoch 1 Batch 429 Loss 9.2819\n",
            "Epoch 1 Batch 858 Loss 8.3650\n",
            "Epoch 1 Loss 8.3621\n",
            "Time taken for 1 epoch: 104.59404969215393 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 7.3125\n",
            "Epoch 2 Batch 429 Loss 6.9981\n",
            "Epoch 2 Batch 858 Loss 6.7640\n",
            "Epoch 2 Loss 6.7628\n",
            "Time taken for 1 epoch: 72.33458352088928 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 6.5978\n",
            "Epoch 3 Batch 429 Loss 6.2346\n",
            "Epoch 3 Batch 858 Loss 6.0809\n",
            "Epoch 3 Loss 6.0804\n",
            "Time taken for 1 epoch: 72.33625173568726 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 5.8739\n",
            "Epoch 4 Batch 429 Loss 5.7267\n",
            "Epoch 4 Batch 858 Loss 5.6128\n",
            "Epoch 4 Loss 5.6118\n",
            "Time taken for 1 epoch: 72.33713245391846 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 5.3210\n",
            "Epoch 5 Batch 429 Loss 5.2753\n",
            "Epoch 5 Batch 858 Loss 5.1803\n",
            "Saving checkpoint for epoch 5 at checkpoints2/ckpt-1\n",
            "Epoch 5 Loss 5.1797\n",
            "Time taken for 1 epoch: 73.44601631164551 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 5.1082\n",
            "Epoch 6 Batch 429 Loss 4.8235\n",
            "Epoch 6 Batch 858 Loss 4.7262\n",
            "Epoch 6 Loss 4.7260\n",
            "Time taken for 1 epoch: 72.33873295783997 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 4.5173\n",
            "Epoch 7 Batch 429 Loss 4.3912\n",
            "Epoch 7 Batch 858 Loss 4.3124\n",
            "Epoch 7 Loss 4.3123\n",
            "Time taken for 1 epoch: 72.33958888053894 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 4.1115\n",
            "Epoch 8 Batch 429 Loss 4.0199\n",
            "Epoch 8 Batch 858 Loss 3.9782\n",
            "Epoch 8 Loss 3.9777\n",
            "Time taken for 1 epoch: 72.35269474983215 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 3.9304\n",
            "Epoch 9 Batch 429 Loss 3.7318\n",
            "Epoch 9 Batch 858 Loss 3.7168\n",
            "Epoch 9 Loss 3.7169\n",
            "Time taken for 1 epoch: 72.34697198867798 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 3.6163\n",
            "Epoch 10 Batch 429 Loss 3.5110\n",
            "Epoch 10 Batch 858 Loss 3.4840\n",
            "Saving checkpoint for epoch 10 at checkpoints2/ckpt-2\n",
            "Epoch 10 Loss 3.4836\n",
            "Time taken for 1 epoch: 73.34115195274353 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 3.1885\n",
            "Epoch 11 Batch 429 Loss 3.2846\n",
            "Epoch 11 Batch 858 Loss 3.2666\n",
            "Epoch 11 Loss 3.2668\n",
            "Time taken for 1 epoch: 72.34741282463074 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 2.9175\n",
            "Epoch 12 Batch 429 Loss 3.0724\n",
            "Epoch 12 Batch 858 Loss 3.0600\n",
            "Epoch 12 Loss 3.0606\n",
            "Time taken for 1 epoch: 72.33566284179688 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 2.9666\n",
            "Epoch 13 Batch 429 Loss 2.8818\n",
            "Epoch 13 Batch 858 Loss 2.8741\n",
            "Epoch 13 Loss 2.8744\n",
            "Time taken for 1 epoch: 72.33881330490112 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 2.5798\n",
            "Epoch 14 Batch 429 Loss 2.7102\n",
            "Epoch 14 Batch 858 Loss 2.7081\n",
            "Epoch 14 Loss 2.7084\n",
            "Time taken for 1 epoch: 72.33683133125305 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 2.6614\n",
            "Epoch 15 Batch 429 Loss 2.5535\n",
            "Epoch 15 Batch 858 Loss 2.5601\n",
            "Saving checkpoint for epoch 15 at checkpoints2/ckpt-3\n",
            "Epoch 15 Loss 2.5606\n",
            "Time taken for 1 epoch: 73.30489540100098 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 2.5607\n",
            "Epoch 16 Batch 429 Loss 2.3976\n",
            "Epoch 16 Batch 858 Loss 2.4222\n",
            "Epoch 16 Loss 2.4228\n",
            "Time taken for 1 epoch: 72.3383481502533 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 2.2931\n",
            "Epoch 17 Batch 429 Loss 2.2758\n",
            "Epoch 17 Batch 858 Loss 2.2979\n",
            "Epoch 17 Loss 2.2987\n",
            "Time taken for 1 epoch: 72.34539842605591 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 2.1858\n",
            "Epoch 18 Batch 429 Loss 2.1577\n",
            "Epoch 18 Batch 858 Loss 2.1859\n",
            "Epoch 18 Loss 2.1859\n",
            "Time taken for 1 epoch: 72.3346176147461 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.7924\n",
            "Epoch 19 Batch 429 Loss 2.0568\n",
            "Epoch 19 Batch 858 Loss 2.0855\n",
            "Epoch 19 Loss 2.0857\n",
            "Time taken for 1 epoch: 72.33968591690063 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.9212\n",
            "Epoch 20 Batch 429 Loss 1.9525\n",
            "Epoch 20 Batch 858 Loss 1.9896\n",
            "Saving checkpoint for epoch 20 at checkpoints2/ckpt-4\n",
            "Epoch 20 Loss 1.9901\n",
            "Time taken for 1 epoch: 73.28371262550354 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 1.8692\n",
            "Epoch 21 Batch 429 Loss 1.8669\n",
            "Epoch 21 Batch 858 Loss 1.9088\n",
            "Epoch 21 Loss 1.9089\n",
            "Time taken for 1 epoch: 72.34060311317444 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 1.7036\n",
            "Epoch 22 Batch 429 Loss 1.7893\n",
            "Epoch 22 Batch 858 Loss 1.8355\n",
            "Epoch 22 Loss 1.8357\n",
            "Time taken for 1 epoch: 72.33648729324341 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 1.6772\n",
            "Epoch 23 Batch 429 Loss 1.7215\n",
            "Epoch 23 Batch 858 Loss 1.7622\n",
            "Epoch 23 Loss 1.7626\n",
            "Time taken for 1 epoch: 72.33761501312256 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 1.6665\n",
            "Epoch 24 Batch 429 Loss 1.6610\n",
            "Epoch 24 Batch 858 Loss 1.7020\n",
            "Epoch 24 Loss 1.7028\n",
            "Time taken for 1 epoch: 72.3431236743927 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 1.4864\n",
            "Epoch 25 Batch 429 Loss 1.5966\n",
            "Epoch 25 Batch 858 Loss 1.6418\n",
            "Saving checkpoint for epoch 25 at checkpoints2/ckpt-5\n",
            "Epoch 25 Loss 1.6422\n",
            "Time taken for 1 epoch: 73.32676792144775 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 1.5111\n",
            "Epoch 26 Batch 429 Loss 1.5323\n",
            "Epoch 26 Batch 858 Loss 1.5904\n",
            "Epoch 26 Loss 1.5908\n",
            "Time taken for 1 epoch: 72.34926795959473 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 1.4598\n",
            "Epoch 27 Batch 429 Loss 1.4927\n",
            "Epoch 27 Batch 858 Loss 1.5404\n",
            "Epoch 27 Loss 1.5409\n",
            "Time taken for 1 epoch: 72.34490966796875 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 1.3929\n",
            "Epoch 28 Batch 429 Loss 1.4434\n",
            "Epoch 28 Batch 858 Loss 1.4924\n",
            "Epoch 28 Loss 1.4926\n",
            "Time taken for 1 epoch: 72.33912253379822 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 1.2495\n",
            "Epoch 29 Batch 429 Loss 1.4005\n",
            "Epoch 29 Batch 858 Loss 1.4530\n",
            "Epoch 29 Loss 1.4535\n",
            "Time taken for 1 epoch: 72.34856486320496 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 1.4662\n",
            "Epoch 30 Batch 429 Loss 1.3590\n",
            "Epoch 30 Batch 858 Loss 1.4163\n",
            "Saving checkpoint for epoch 30 at checkpoints2/ckpt-6\n",
            "Epoch 30 Loss 1.4164\n",
            "Time taken for 1 epoch: 73.35658693313599 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 1.2140\n",
            "Epoch 31 Batch 429 Loss 1.3282\n",
            "Epoch 31 Batch 858 Loss 1.3782\n",
            "Epoch 31 Loss 1.3786\n",
            "Time taken for 1 epoch: 72.34738969802856 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 1.2416\n",
            "Epoch 32 Batch 429 Loss 1.2864\n",
            "Epoch 32 Batch 858 Loss 1.3430\n",
            "Epoch 32 Loss 1.3431\n",
            "Time taken for 1 epoch: 72.33806371688843 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 1.3055\n",
            "Epoch 33 Batch 429 Loss 1.2476\n",
            "Epoch 33 Batch 858 Loss 1.3115\n",
            "Epoch 33 Loss 1.3120\n",
            "Time taken for 1 epoch: 72.34944939613342 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 1.2385\n",
            "Epoch 34 Batch 429 Loss 1.2231\n",
            "Epoch 34 Batch 858 Loss 1.2835\n",
            "Epoch 34 Loss 1.2836\n",
            "Time taken for 1 epoch: 72.33912396430969 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 1.2104\n",
            "Epoch 35 Batch 429 Loss 1.1884\n",
            "Epoch 35 Batch 858 Loss 1.2552\n",
            "Saving checkpoint for epoch 35 at checkpoints2/ckpt-7\n",
            "Epoch 35 Loss 1.2557\n",
            "Time taken for 1 epoch: 73.371333360672 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 1.1280\n",
            "Epoch 36 Batch 429 Loss 1.1575\n",
            "Epoch 36 Batch 858 Loss 1.2258\n",
            "Epoch 36 Loss 1.2264\n",
            "Time taken for 1 epoch: 72.33789587020874 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 1.1147\n",
            "Epoch 37 Batch 429 Loss 1.1315\n",
            "Epoch 37 Batch 858 Loss 1.1990\n",
            "Epoch 37 Loss 1.1995\n",
            "Time taken for 1 epoch: 72.34729027748108 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 1.1468\n",
            "Epoch 38 Batch 429 Loss 1.1159\n",
            "Epoch 38 Batch 858 Loss 1.1794\n",
            "Epoch 38 Loss 1.1798\n",
            "Time taken for 1 epoch: 72.33813571929932 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.9773\n",
            "Epoch 39 Batch 429 Loss 1.0918\n",
            "Epoch 39 Batch 858 Loss 1.1567\n",
            "Epoch 39 Loss 1.1570\n",
            "Time taken for 1 epoch: 72.34275531768799 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 1.0070\n",
            "Epoch 40 Batch 429 Loss 1.0727\n",
            "Epoch 40 Batch 858 Loss 1.1372\n",
            "Saving checkpoint for epoch 40 at checkpoints2/ckpt-8\n",
            "Epoch 40 Loss 1.1376\n",
            "Time taken for 1 epoch: 73.3353009223938 secs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_state()\n",
        "    count = 0\n",
        "    for (batch, (inp, tar)) in enumerate(ds):\n",
        "        train_step(inp, tar)\n",
        "\n",
        "        if batch % 429 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
        "\n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
        "\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVbEUCZagJ0G"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMbqGTixu1cl"
      },
      "source": [
        "#### Predicting one word at a time at the decoder and appending it to the output; then taking the complete sequence as an input to the decoder and repeating until maxlen or stop keyword appears"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5D5cv2Jd8-6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def evaluate(input_news):\n",
        "    input_news = news_tokenizer.texts_to_sequences([input_news])\n",
        "    input_news = tf.keras.preprocessing.sequence.pad_sequences(input_news, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "\n",
        "    encoder_input = tf.expand_dims(input_news[0], 0)\n",
        "\n",
        "    decoder_input = [summary_tokenizer.word_index['<start>']]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "    for i in range(decoder_maxlen):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input,\n",
        "            output,\n",
        "            training=False, #Passing as a Keyword Argument\n",
        "            enc_padding_mask=enc_padding_mask,\n",
        "            look_ahead_mask=combined_mask,\n",
        "            dec_padding_mask=dec_padding_mask\n",
        "        )\n",
        "\n",
        "        predictions = predictions[: ,-1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if predicted_id == summary_tokenizer.word_index['<end>']:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkpdiW6wnmiS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def summarize(input_news):\n",
        "    summarized = evaluate(input_news=input_news)[0].numpy()\n",
        "    summarized = np.expand_dims(summarized[1:], 0)\n",
        "    return summary_tokenizer.sequences_to_texts(summarized)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Azfs-rvlTfOd"
      },
      "outputs": [],
      "source": [
        "def checkinbulk(randomnumber):\n",
        "  print('Actual summary:', summarize(news[randomnumber]))\n",
        "\n",
        "  print('News: ', news[randomnumber])\n",
        "  print('Actual summary: ', summary[randomnumber][7:-6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_FUA-pqTfP6",
        "outputId": "0d07979e-467b-4386-e9db-120ec53c6155"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Actual summary: hp announces price hike across india\n",
            "News:  american technology company hp has announced an increase in the list prices of its products by across categories in the indian market hp said the hike is in line with its move to adjust to currency movement and commodity prices globally the company leads the indian pc market with an overall market share of\n",
            "Actual summary:   hp announces price hike across products in india\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "random_number = random.randint(0, 55104)\n",
        "\n",
        "checkinbulk(random_number)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
